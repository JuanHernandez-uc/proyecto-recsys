{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRn7tAtvznt-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_behaviors = pd.read_csv(\"behaviors.tsv\", sep=\"\\t\", names=['ImpressionID', 'UserID', 'Time', 'History', 'Impressions'])\n",
        "df_news = pd.read_csv(\"news.tsv\", sep=\"\\t\", names=['NewsID', 'Category', 'SubCategory', 'Title', 'Abstract', 'URL', 'TitleEntities', 'AbstractEntities'])"
      ],
      "metadata": {
        "id": "1tPZIFG41D58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_behaviors.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-d0mvuo1Vn_",
        "outputId": "3b49174f-80dc-4b41-8370-b5e8a7cb5651"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(156965, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_behaviors[\"Time\"] = pd.to_datetime(df_behaviors[\"Time\"])\n",
        "cutoff = pd.to_datetime(\"2019-11-14\")\n",
        "\n",
        "behavior_train = df_behaviors[df_behaviors[\"Time\"] < cutoff].copy()\n",
        "behavior_val   = df_behaviors[df_behaviors[\"Time\"] >= cutoff].copy()"
      ],
      "metadata": {
        "id": "65S_yx1t1Wln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Usando dispositivo: {device}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TW6Z1YS1XjO",
        "outputId": "d482dc60-1d70-47b9-cded-8bf3489bf193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usando dispositivo: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    tokens = re.findall(r\"[\\w']+\", text.lower())\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "H50xqCYK1YhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "longitudes = df_news[\"Title\"].dropna().apply(lambda x: len(x.split()))\n",
        "cantidad_menor_20 = (longitudes < 20).sum()\n",
        "total = len(longitudes)\n",
        "\n",
        "print(f\"Títulos con menos de 20 palabras: {cantidad_menor_20} de {total} ({cantidad_menor_20 / total:.2%})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qE9Fzw-J1ZZO",
        "outputId": "61fa6d19-e4d4-456a-fc2c-050714ead421"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Títulos con menos de 20 palabras: 50633 de 51282 (98.73%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
        "idx = 2 # Por <UNK> y <PAD>\n",
        "news2idx = {}  # Mapeo: news_id -> lista de índices de palabras (padded/trunc)\n",
        "max_size_title = 20"
      ],
      "metadata": {
        "id": "sWUBzwkT1ahW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _, row in tqdm(df_news.iterrows(), total=df_news.shape[0]):\n",
        "    news_id = row[\"NewsID\"]\n",
        "    title = row[\"Title\"]\n",
        "    tokens = [] if pd.isna(title) else tokenize(title)\n",
        "    token_idxs = []\n",
        "    for w in tokens[:max_size_title]:  # truncar título largo\n",
        "        if w not in word2idx:\n",
        "            word2idx[w] = idx\n",
        "            idx += 1\n",
        "        token_idxs.append(word2idx.get(w, word2idx['<UNK>']))\n",
        "    # Rellenar con PAD si es más corto que title_max\n",
        "    if len(token_idxs) < max_size_title:\n",
        "        token_idxs += [word2idx['<PAD>']] * (max_size_title - len(token_idxs))\n",
        "    news2idx[news_id] = token_idxs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upo5CjpX1bW-",
        "outputId": "129ce396-bca8-465d-a6a2-87cfa91c7e03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 51282/51282 [00:02<00:00, 18283.59it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(word2idx)\n",
        "print(f'Vocabulario: {vocab_size} palabras')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUGgNd7o1can",
        "outputId": "3b7c1b98-88ea-4d89-869e-ecad350de768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulario: 37272 palabras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "for _, row in tqdm(behavior_train.iterrows(), total=behavior_train.shape[0]):\n",
        "    hist_str = row['History']\n",
        "    hist_ids = [] if pd.isna(hist_str) else [nid for nid in hist_str.split() if nid]\n",
        "    impr = row['ImpressionID']\n",
        "    imps = [] if pd.isna(row['Impressions']) else row['Impressions'].split()\n",
        "    for imp in imps:\n",
        "        if len(imp) == 0:\n",
        "            continue\n",
        "        parts = imp.split('-')\n",
        "        if len(parts) != 2:\n",
        "            continue\n",
        "        news_id, click = parts[0], parts[1]\n",
        "        label = int(click)\n",
        "        data.append((impr, hist_ids, news_id, label))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFUNeOhO1djW",
        "outputId": "09d7716e-af3e-4549-a2e8-c4905c4d074f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 126695/126695 [00:15<00:00, 8107.56it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Total de ejemplos de interacción: {len(data)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAJKAq511e2m",
        "outputId": "69ba8dc1-b18c-494a-b11d-c62cebaa63e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de ejemplos de interacción: 4621015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_data = []\n",
        "\n",
        "for _, row in tqdm(behavior_val.iterrows(), total=behavior_val.shape[0]):\n",
        "    hist_str = row['History']\n",
        "    hist_ids = [] if pd.isna(hist_str) else [nid for nid in hist_str.split() if nid]\n",
        "    impr = row['ImpressionID']\n",
        "    imps = row['Impressions'].split()\n",
        "    for imp in imps:\n",
        "        if len(imp) == 0:\n",
        "            continue\n",
        "        parts = imp.split('-')\n",
        "        if len(parts) != 2:\n",
        "            continue\n",
        "        news_id, click = parts[0], parts[1]\n",
        "        val_data.append((impr, hist_ids, news_id, int(click)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p30Gw8JU1f7O",
        "outputId": "cac6d1f3-951e-4991-b05c-64fc10cbe961"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30270/30270 [00:03<00:00, 8235.70it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Total ejemplos validación: {len(val_data)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4dB2RD61hH2",
        "outputId": "29695a93-511d-4d90-92e8-80ade46e2141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total ejemplos validación: 1222429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MINDListDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Devuelve:\n",
        "        hist_tensor  : [hist_max, title_max]\n",
        "        cand_tensor  : [C,         title_max]\n",
        "        label_tensor : [C]  (0/1, un solo 1)\n",
        "        impr_id      : str\n",
        "    \"\"\"\n",
        "    def __init__(self, interactions, news2idx, word2idx,\n",
        "                 hist_max, title_max):\n",
        "        self.news2idx  = news2idx\n",
        "        self.word2idx  = word2idx\n",
        "        self.hist_max  = hist_max\n",
        "        self.title_max = title_max\n",
        "\n",
        "        # Agrupar ejemplos por impresión -----------------------------\n",
        "        sessions = defaultdict(list)\n",
        "        for impr, hist_ids, cand_id, label in interactions:\n",
        "            sessions[impr].append((hist_ids, cand_id, label))\n",
        "        self.impr_ids = list(sessions.keys())\n",
        "        self.sessions = sessions\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.impr_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        impr   = self.impr_ids[idx]\n",
        "        triples = self.sessions[impr]           # lista de (hist, cand, label)\n",
        "\n",
        "        # -------- historial (todos los candidatos comparten el mismo) -----\n",
        "        hist_ids = triples[0][0][-self.hist_max:]        # recorte por la derecha\n",
        "        hist_seq = [self.news2idx.get(nid,\n",
        "                    [self.word2idx['<PAD>']]*self.title_max) for nid in hist_ids]\n",
        "        while len(hist_seq) < self.hist_max:\n",
        "            hist_seq.insert(0, [self.word2idx['<PAD>']]*self.title_max)\n",
        "\n",
        "        # -------- candidatos + etiquetas -------------------------------\n",
        "        cand_seqs, labels = [], []\n",
        "        for _, cand_id, lbl in triples:\n",
        "            cand_seqs.append(\n",
        "                self.news2idx.get(cand_id,\n",
        "                     [self.word2idx['<PAD>']]*self.title_max))\n",
        "            labels.append(lbl)\n",
        "\n",
        "        return (torch.tensor(hist_seq,  dtype=torch.long),        # [H,L]\n",
        "                torch.tensor(cand_seqs, dtype=torch.long),        # [C,L]\n",
        "                torch.tensor(labels,   dtype=torch.float),        # [C]\n",
        "                impr)"
      ],
      "metadata": {
        "id": "ir04iVO71jUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn_list(batch):\n",
        "    \"\"\"\n",
        "    Devuelve:\n",
        "        hist_batch  : [B, H, L]\n",
        "        cand_batch  : [B, C_max, L]\n",
        "        label_batch : [B, C_max]  (0/1, padded con -1)\n",
        "        mask_batch  : [B, C_max]  (True donde existe candidato)\n",
        "        impr_batch  : list[str]\n",
        "    \"\"\"\n",
        "    hist_list, cand_list, label_list, impr_list = zip(*batch)\n",
        "\n",
        "    # Historial: tamaño fijo\n",
        "    hist_batch = torch.stack(hist_list)             # [B,H,L]\n",
        "\n",
        "    # Candidatos: pad al máximo C del batch\n",
        "    C_max = max(x.size(0) for x in cand_list)\n",
        "    L     = cand_list[0].size(1)\n",
        "    pad_val = 0  # token PAD\n",
        "\n",
        "    cand_pad   = torch.full((len(batch), C_max, L), pad_val, dtype=torch.long)\n",
        "    label_pad  = torch.full((len(batch), C_max),    -1,      dtype=torch.float)\n",
        "    mask_pad   = torch.zeros(len(batch), C_max,     dtype=torch.bool)\n",
        "\n",
        "    for i,(cands, labels) in enumerate(zip(cand_list, label_list)):\n",
        "        C = cands.size(0)\n",
        "        cand_pad[i,:C]  = cands\n",
        "        label_pad[i,:C] = labels\n",
        "        mask_pad[i,:C]  = 1\n",
        "\n",
        "    return (hist_batch.to(device),\n",
        "            cand_pad.to(device),\n",
        "            label_pad.to(device),\n",
        "            mask_pad.to(device),\n",
        "            list(impr_list))\n"
      ],
      "metadata": {
        "id": "euYhosJa1kr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_hist_title = 50\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "4wqU253z1lsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MINDListDataset(data, news2idx, word2idx, max_hist_title, max_size_title)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_list)\n",
        "\n",
        "val_dataset = MINDListDataset(val_data, news2idx, word2idx, max_hist_title, max_size_title)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_list)"
      ],
      "metadata": {
        "id": "zlFMx44z1mwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 300\n",
        "num_heads = 20\n",
        "lr = 0.001"
      ],
      "metadata": {
        "id": "Lh5_7mMH1nyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove = True\n",
        "\n",
        "if glove:\n",
        "    embedding_matrix = np.random.normal(scale=0.6, size=(vocab_size, embed_dim))\n",
        "    found = 0\n",
        "    with open(\"glove.6B.300d.txt\", 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            parts = line.rstrip().split(' ')\n",
        "            word = parts[0]\n",
        "            if word in word2idx:\n",
        "                vec = np.array(parts[1:], dtype=np.float32)\n",
        "                if vec.shape[0] == embed_dim:\n",
        "                    embedding_matrix[word2idx[word]] = vec\n",
        "                    found += 1\n",
        "    print(f'Palabras encontradas en GloVe: {found}/{vocab_size}')\n",
        "    embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float)\n",
        "else:\n",
        "    embedding_matrix = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5iMvqeX1o03",
        "outputId": "5465d1f2-5293-45de-e0f4-988778bbeb66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palabras encontradas en GloVe: 28233/37272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FastformerAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Atención Fastformer (Atención aditiva global) que reemplaza nn.MultiheadAttention.\n",
        "    Opera con entradas de forma (L, B, E) o (B, L, E), realizando la proyección Q, K, V\n",
        "    por separado, obteniendo vectores globales y propagando interacciones por producto\n",
        "    elemento a elemento, según Fastformer (Fastformer: Additive Attention Can Be All You Need).\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0.0):\n",
        "        super(FastformerAttention, self).__init__()\n",
        "        assert embed_dim % num_heads == 0, \"embed_dim debe ser divisible por num_heads\"\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        # Proyecciones lineales para Q, K, V (similar a multi-cabeza estándar)\n",
        "        self.W_q = nn.Linear(embed_dim, embed_dim, bias=True)\n",
        "        self.W_k = nn.Linear(embed_dim, embed_dim, bias=True)\n",
        "        self.W_v = nn.Linear(embed_dim, embed_dim, bias=True)\n",
        "        # Parámetros de atención aditiva por cabeza (vectores de peso para Q y K)\n",
        "        # Formato (num_heads, head_dim) para aplicar dot-product con cada vector de dimensión head_dim\n",
        "        self.attn_wq = nn.Parameter(torch.Tensor(num_heads, self.head_dim))\n",
        "        self.attn_wk = nn.Parameter(torch.Tensor(num_heads, self.head_dim))\n",
        "        # Capa de salida tras concatenar cabezas\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # Inicialización\n",
        "        nn.init.xavier_uniform_(self.W_q.weight)\n",
        "        nn.init.xavier_uniform_(self.W_k.weight)\n",
        "        nn.init.xavier_uniform_(self.W_v.weight)\n",
        "        nn.init.xavier_uniform_(self.out_proj.weight)\n",
        "        nn.init.zeros_(self.W_q.bias)\n",
        "        nn.init.zeros_(self.W_k.bias)\n",
        "        nn.init.zeros_(self.W_v.bias)\n",
        "        nn.init.zeros_(self.out_proj.bias)\n",
        "        nn.init.xavier_uniform_(self.attn_wq)\n",
        "        nn.init.xavier_uniform_(self.attn_wk)\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        \"\"\"\n",
        "        query, key, value: tensores de forma (L, B, E) ó (S, N, E) donde\n",
        "        L=longitud de secuencia, B=batch, E=embed_dim.\n",
        "        Fastformer es simétrico en q=k=v, pero aceptamos tres argumentos para compatibilidad.\n",
        "        \"\"\"\n",
        "        # Permutar para batch-first: [B, L, E]\n",
        "        transpose = False\n",
        "        if query.dim() == 3 and query.shape[0] != query.shape[1]:\n",
        "            # Asumimos forma (L, B, E)\n",
        "            query = query.transpose(0, 1)\n",
        "            key = key.transpose(0, 1)\n",
        "            value = value.transpose(0, 1)\n",
        "            transpose = True\n",
        "        # Proyectar Q, K, V\n",
        "        # Ahora shapes: [B, L, E]\n",
        "        Q = self.W_q(query)   # [B, L, E]\n",
        "        K = self.W_k(key)     # [B, L, E]\n",
        "        V = self.W_v(value)   # [B, L, E]\n",
        "        B, L, E = Q.size()\n",
        "        H = self.num_heads\n",
        "        D = self.head_dim\n",
        "        # Dividir en cabezas: [B, L, H, D]\n",
        "        Q = Q.view(B, L, H, D)\n",
        "        K = K.view(B, L, H, D)\n",
        "        V = V.view(B, L, H, D)\n",
        "        # Reordenar para [B, H, L, D]\n",
        "        Q = Q.permute(0, 2, 1, 3)\n",
        "        K = K.permute(0, 2, 1, 3)\n",
        "        V = V.permute(0, 2, 1, 3)\n",
        "        # =========== Fastformer Steps ===========\n",
        "        # 1) Atención aditiva sobre Q para obtener q_global [B, H, D]\n",
        "        # Calculamos puntuaciones: sum_{j}( w_q[h,j] * Q[...,j] )\n",
        "        # w_q: [H, D], Q: [B, H, L, D]\n",
        "        # Producto elemento a elemento y sumar sobre dimensión D:\n",
        "        # scores_q: [B, H, L]\n",
        "        scores_q = (Q * self.attn_wq.unsqueeze(0).unsqueeze(2)).sum(dim=-1)  # [B, H, L]\n",
        "        alpha = torch.softmax(scores_q, dim=-1)  # [B, H, L]\n",
        "        # Obtener vector q_global: suma ponderada de Q sobre L\n",
        "        # alpha: [B, H, L], Q: [B, H, L, D] -> q_global: [B, H, D]\n",
        "        q_global = torch.einsum('bhl,bhld->bhd', alpha, Q)\n",
        "        # 2) Interactuar q_global con cada K por producto elemento a elemento -> K'\n",
        "        # Extendemos q_global para cada posición L: [B, H, 1, D] * [B, H, L, D] -> [B, H, L, D]\n",
        "        K_prime = q_global.unsqueeze(2) * K  # [B, H, L, D]\n",
        "        # 3) Atención aditiva sobre K_prime para obtener k_global [B, H, D]\n",
        "        scores_k = (K_prime * self.attn_wk.unsqueeze(0).unsqueeze(2)).sum(dim=-1)  # [B, H, L]\n",
        "        beta = torch.softmax(scores_k, dim=-1)  # [B, H, L]\n",
        "        k_global = torch.einsum('bhl,bhld->bhd', beta, K_prime)  # [B, H, D]\n",
        "        # 4) Interactuar k_global con cada V -> V'\n",
        "        V_prime = k_global.unsqueeze(2) * V  # [B, H, L, D]\n",
        "        # Rearmar V' combinando cabezas: [B, H, L, D] -> [B, L, H*D]\n",
        "        V_prime = V_prime.permute(0, 2, 1, 3).contiguous().view(B, L, H * D)  # [B, L, E]\n",
        "        # Capa lineal de salida y agregar Q (residuo)\n",
        "        out = self.out_proj(V_prime)  # [B, L, E]\n",
        "        # Capa residual: sumamos la proyección original de Q antes de dividir cabezas\n",
        "        # Primero reconstruir Q original (bidimensional por cada posición)\n",
        "        Q_orig = Q.permute(0, 2, 1, 3).contiguous().view(B, L, H * D)  # [B, L, E]\n",
        "        out = out + Q_orig\n",
        "        # Opcional: aplicar dropout\n",
        "        out = self.dropout(out)\n",
        "        # Devolver en forma (L, B, E)\n",
        "        if transpose:\n",
        "            out = out.transpose(0, 1).contiguous()\n",
        "        return out"
      ],
      "metadata": {
        "id": "gkNJSt3e1pUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NewsEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Codificador de noticias: procesa títulos de noticias (secuencias de tokens)\n",
        "    y produce vectores de noticia. Reemplaza la atención multi-cabeza por FastformerAttention.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, title_max, pretrained_emb=None):\n",
        "        super(NewsEncoder, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.title_max = title_max\n",
        "        # Capa de embedding de palabras\n",
        "        self.word_embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "\n",
        "        if pretrained_emb is not None:\n",
        "            self.word_embedding.weight.data.copy_(pretrained_emb)\n",
        "            self.word_embedding.weight.requires_grad = True  # o False si no quieres fine-tune\n",
        "\n",
        "        # Capa convolucional 1D para extraer características locales de palabras (opcional, similar a arquitectura original)\n",
        "        # Usamos múltiples filtros 1xD para captar n-gramas de tamaño 3 por ejemplo\n",
        "        self.conv = nn.Conv1d(in_channels=embed_dim, out_channels=embed_dim, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        # Atención Fastformer sobre la secuencia de características de palabras\n",
        "        self.self_attn = FastformerAttention(embed_dim, num_heads, dropout=0.1)\n",
        "        # Atención aditiva para agregar las palabras importantes en el título\n",
        "        self.attn_vector = nn.Linear(embed_dim, 1)  # para puntuación de cada palabra\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: tensor de tokens de noticias con forma [B, title_max].\n",
        "        Devuelve: vectores de noticias de forma [B, embed_dim].\n",
        "        \"\"\"\n",
        "        # Embedding y extracción de características locales\n",
        "        # Palabras: [B, title_max, E] -> conv espera [B, E, title_max]\n",
        "        emb = self.word_embedding(x)            # [B, L, E]\n",
        "        emb = emb.transpose(1, 2)               # [B, E, L]\n",
        "        conv_out = self.relu(self.conv(emb))    # [B, E, L]\n",
        "        conv_out = conv_out.transpose(1, 2)     # [B, L, E]\n",
        "        # Atención Fastformer (auto-atención) entre las posiciones de palabras\n",
        "        # FastformerAttention espera (L, B, E) o (B, L, E); adaptamos:\n",
        "        conv_out_trans = conv_out.transpose(0, 1).contiguous()  # [L, B, E]\n",
        "        attn_out = self.self_attn(conv_out_trans, conv_out_trans, conv_out_trans)  # [L, B, E]\n",
        "        attn_out = attn_out.transpose(0, 1)  # [B, L, E]\n",
        "        # Atención aditiva para obtener vector final de noticia\n",
        "        # Calcular puntuación de importancia para cada palabra\n",
        "        scores = self.attn_vector(attn_out).squeeze(-1)  # [B, L]\n",
        "        weights = self.softmax(scores)                    # [B, L]\n",
        "        news_vector = torch.bmm(weights.unsqueeze(1), attn_out).squeeze(1)  # [B, E]\n",
        "        return news_vector  # [B, embed_dim]\n",
        "\n",
        "class UserEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Codificador de usuario: agrega vectores de noticias historiales usando Fastformer.\n",
        "    Toma un historial de noticias y devuelve un vector de usuario.\n",
        "    \"\"\"\n",
        "    def __init__(self, news_encoder, embed_dim, num_heads, hist_max):\n",
        "        super(UserEncoder, self).__init__()\n",
        "        self.news_encoder = news_encoder  # instancia de NewsEncoder para codificar cada noticia\n",
        "        self.hist_max = hist_max\n",
        "        # Atención Fastformer sobre la secuencia de vectores de noticia del historial\n",
        "        self.self_attn = FastformerAttention(embed_dim, num_heads, dropout=0.1)\n",
        "        # Atención aditiva para agregar las noticias más relevantes del historial\n",
        "        self.attn_vector = nn.Linear(embed_dim, 1)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, hist_x):\n",
        "        \"\"\"\n",
        "        hist_x: tensor de tokens de noticias de historial con forma [B, hist_max, title_max].\n",
        "        Devuelve: vector de usuario de forma [B, embed_dim].\n",
        "        \"\"\"\n",
        "        B, H, L = hist_x.size()\n",
        "        # Codificar cada noticia en el historial\n",
        "        hist_x_flat = hist_x.view(B * H, L)                  # [B*H, title_max]\n",
        "        news_vectors = self.news_encoder(hist_x_flat)        # [B*H, embed_dim]\n",
        "        news_vectors = news_vectors.view(B, H, -1)           # [B, hist_max, embed_dim]\n",
        "        # Atención Fastformer sobre las noticias del historial\n",
        "        nv_trans = news_vectors.transpose(0, 1).contiguous() # [H, B, E]\n",
        "        attn_out = self.self_attn(nv_trans, nv_trans, nv_trans)  # [H, B, E]\n",
        "        attn_out = attn_out.transpose(0, 1)                  # [B, H, E]\n",
        "        # Atención aditiva para agregar vectores de noticias importantes\n",
        "        scores = self.attn_vector(attn_out).squeeze(-1)      # [B, H]\n",
        "        weights = self.softmax(scores)                       # [B, H]\n",
        "        user_vector = torch.bmm(weights.unsqueeze(1), attn_out).squeeze(1)  # [B, E]\n",
        "        return user_vector  # [B, embed_dim]"
      ],
      "metadata": {
        "id": "JPmn3pVg11NZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FastformerNRMS(nn.Module):\n",
        "    \"\"\"\n",
        "    Modelo NRMS modificado con Fastformer.\n",
        "    Mantiene la misma interfaz: forward(hist_tensor, cand_tensor).\n",
        "    hist_tensor: [B, hist_max, title_max], cand_tensor: [B, cand_count, title_max].\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, title_max, hist_max, pretrained_emb=None):\n",
        "        super(FastformerNRMS, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.title_max = title_max\n",
        "        self.hist_max = hist_max\n",
        "        # News encoder y user encoder con Fastformer\n",
        "        self.news_encoder = NewsEncoder(vocab_size, embed_dim, num_heads, title_max, pretrained_emb)\n",
        "        self.user_encoder = UserEncoder(self.news_encoder, embed_dim, num_heads, hist_max)\n",
        "        # (Opcional) proyección final o dropout\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, hist_tensor, cand_tensor, mask=None):\n",
        "        \"\"\"\n",
        "        hist_tensor : [B, hist_max, title_max]\n",
        "        cand_tensor : [B, C,       title_max]\n",
        "        mask        : [B, C]  (bool) – True donde el candidato existe\n",
        "        \"\"\"\n",
        "        B, H, L = hist_tensor.size()\n",
        "        _, C, _ = cand_tensor.size()\n",
        "        # Codificar usuario\n",
        "        user_vector = self.user_encoder(hist_tensor)            # [B, E]\n",
        "        # Codificar candidatos (aplicar NewsEncoder a cada candidato)\n",
        "        cand_flat = cand_tensor.view(B * C, L)                  # [B*C, title_max]\n",
        "        cand_vecs = self.news_encoder(cand_flat)               # [B*C, E]\n",
        "        cand_vecs = cand_vecs.view(B, C, -1)                   # [B, cand_count, E]\n",
        "        # Calcular similaridad (producto punto usuario con cada candidato)\n",
        "        # Expandir user_vector para combinar con candidatos\n",
        "\n",
        "        cand_vecs = self.dropout(cand_vecs)\n",
        "        user_vector = self.dropout(user_vector)\n",
        "\n",
        "        user_exp = user_vector.unsqueeze(1)                    # [B, 1, E]\n",
        "        logits = torch.sum(cand_vecs * user_exp, dim=-1)       # [B, cand_count]\n",
        "\n",
        "        if mask is not None:\n",
        "            logits = logits.masked_fill(~mask, -1e9)             # -∞ donde no hay candidato\n",
        "\n",
        "        return logits                                            # [B,C]"
      ],
      "metadata": {
        "id": "TyZkcEl512j7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastformerNRMS(vocab_size, embed_dim, num_heads, max_size_title, max_hist_title,\n",
        "             pretrained_emb=embedding_matrix.to(device) if embedding_matrix is not None else None)\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "ytcEQxLA14d_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def ndcg_score(labels, scores, k=5):\n",
        "    order = np.argsort(scores)[::-1]\n",
        "    labels = np.array(labels)\n",
        "    dcg = 0.0\n",
        "    for i in range(min(k, len(labels))):\n",
        "        rel = labels[order[i]]\n",
        "        dcg += (2**rel - 1) / np.log2(i+2)\n",
        "    ideal = np.sort(labels)[::-1]\n",
        "    idcg = 0.0\n",
        "    for i in range(min(k, int(np.sum(labels)))):\n",
        "        idcg += 1.0 / np.log2(i+2)\n",
        "    return dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "def mrr_score(labels, scores):\n",
        "    order = np.argsort(scores)[::-1]\n",
        "    labels = np.array(labels)[order]\n",
        "    for rank, label in enumerate(labels, start=1):\n",
        "        if label == 1:\n",
        "            return 1.0 / rank\n",
        "    return 0.0"
      ],
      "metadata": {
        "id": "_tmaqomD2tuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 3"
      ],
      "metadata": {
        "id": "q64WVk6a2xjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_ndcg5 = 0.0\n",
        "best_model_state = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for hist_batch, cand_batch, label_batch, mask_batch, _ in tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\"):\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(hist_batch, cand_batch, mask_batch)\n",
        "        target = label_batch.argmax(dim=1)        # [B]\n",
        "        loss = criterion(logits, target)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch} - Pérdida promedio: {avg_loss:.4f}\")\n",
        "\n",
        "    # Evaluación en validación\n",
        "    if val_loader is None:\n",
        "        continue\n",
        "\n",
        "    model.eval()\n",
        "    ndcg5_list, ndcg10_list, mrr_list, auc_list = [], [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for hist_batch, cand_batch, label_batch, mask_batch, impr_batch in val_loader:\n",
        "            logits  = model(hist_batch, cand_batch, mask_batch)  # [B,C]\n",
        "            scores  = logits.cpu().numpy()\n",
        "            labels  = label_batch.cpu().numpy()\n",
        "            masks   = mask_batch.cpu().numpy()\n",
        "\n",
        "            for s, l, m in zip(scores, labels, masks):\n",
        "                # recortar a candidatos reales\n",
        "                s = s[m]        # (C_real,)\n",
        "                l = l[m]        # (C_real,)\n",
        "\n",
        "                ndcg5_list .append(ndcg_score(l, s, k=5))\n",
        "                ndcg10_list.append(ndcg_score(l, s, k=10))\n",
        "                mrr_list  .append(mrr_score(l, s))\n",
        "                if l.max() > 0 and l.min() == 0:     # al menos 1 pos y 1 neg\n",
        "                    auc_list.append(roc_auc_score(l, s))\n",
        "\n",
        "    ndcg5  = np.mean(ndcg5_list)\n",
        "    ndcg10 = np.mean(ndcg10_list)\n",
        "    mrr    = np.mean(mrr_list)\n",
        "    auc    = np.mean(auc_list) if auc_list else 0.0\n",
        "\n",
        "    if ndcg5 > best_ndcg5:\n",
        "        best_ndcg5 = ndcg5\n",
        "        best_model_state = model.state_dict()\n",
        "        print(f\"» Nuevo mejor modelo guardado (nDCG@5 = {ndcg5:.4f})\")\n",
        "\n",
        "    print(f\"Validación – AUC: {auc:.4f} | MRR: {mrr:.4f} | \"\n",
        "          f\"nDCG@5: {ndcg5:.4f} | nDCG@10: {ndcg10:.4f}\")\n",
        "\n",
        "if best_model_state is not None:\n",
        "    torch.save(best_model_state, \"nrms_fastformer_best.pt\")\n",
        "    print(\"Modelo con mejor nDCG@5 guardado en nrms_fastformer_best.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duLvr7fm2yjP",
        "outputId": "23a230af-f059-4102-b2bb-dfc79ca83309"
      },
      "execution_count": 27,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/3: 100%|██████████| 3960/3960 [17:11<00:00,  3.84it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Pérdida promedio: 2.7985\n",
            "» Nuevo mejor modelo guardado (nDCG@5 = 0.3055)\n",
            "Validación – AUC: 0.6429 | MRR: 0.3330 | nDCG@5: 0.3055 | nDCG@10: 0.3661\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3: 100%|██████████| 3960/3960 [17:06<00:00,  3.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 - Pérdida promedio: 2.8410\n",
            "» Nuevo mejor modelo guardado (nDCG@5 = 0.3103)\n",
            "Validación – AUC: 0.6498 | MRR: 0.3363 | nDCG@5: 0.3103 | nDCG@10: 0.3712\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3: 100%|██████████| 3960/3960 [17:04<00:00,  3.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 - Pérdida promedio: 3.4581\n",
            "Validación – AUC: 0.6473 | MRR: 0.3309 | nDCG@5: 0.3054 | nDCG@10: 0.3683\n",
            "Modelo con mejor nDCG@5 guardado en nrms_fastformer_best.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def construir_seq2title(news2idx, idx2word, pad_idx=PAD):\n",
        "    seq2title = {}\n",
        "    for token_ids in news2idx.values():\n",
        "        seq_key = tuple(token_ids)                         # <-- clave exactamente igual a lo que entrega el DataLoader\n",
        "        words = [idx2word[i] for i in token_ids if i != pad_idx]\n",
        "        seq2title[seq_key] = \" \".join(words)\n",
        "    return seq2title"
      ],
      "metadata": {
        "id": "YY60zcfeB6Op"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mostrar_perfil_y_recomendaciones(model, dataloader, seq2title, device, n=3, hist_k=5, stop_on_click=True):\n",
        "    model.eval()\n",
        "    ejemplos_mostrados = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for hist_batch, cand_batch, label_batch, mask_batch, _ in dataloader:\n",
        "\n",
        "            # Mover todo a device\n",
        "            hist_batch, cand_batch = hist_batch.to(device), cand_batch.to(device)\n",
        "            label_batch, mask_batch = label_batch.to(device), mask_batch.to(device)\n",
        "\n",
        "            logits = model(hist_batch, cand_batch, mask_batch)\n",
        "            probs  = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "            labels = label_batch.cpu().numpy()\n",
        "            masks  = mask_batch.cpu().numpy()  # 1 = candidato real, 0 = padding\n",
        "\n",
        "            for i in range(len(hist_batch)):\n",
        "                print(f\"\\n--- Usuario {ejemplos_mostrados+1} ---\")\n",
        "\n",
        "                # -------- historial (solo hist_k primeros) ----------\n",
        "                print(\"Historial (hasta {} ítems):\".format(hist_k))\n",
        "                contador = 0\n",
        "                for seq in hist_batch[i].cpu().numpy():\n",
        "                    titulo = seq2title.get(tuple(seq), None)\n",
        "                    if titulo:\n",
        "                        print(f\" - {titulo}\")\n",
        "                        contador += 1\n",
        "                        if contador >= hist_k:\n",
        "                            break\n",
        "\n",
        "                # -------- candidatos (hasta encontrar un “✔” si stop_on_click) ----------\n",
        "                print(\"\\nCandidatos (ordenados por score):\")\n",
        "                scores    = probs[i]\n",
        "                cand_seqs = cand_batch[i].cpu().numpy()\n",
        "                mask_line = masks[i].astype(bool)      # True para candidatos reales\n",
        "\n",
        "                # 1) ordenar índices de candidatos reales por score descendente\n",
        "                valid_idx = scores[mask_line].argsort()[::-1]\n",
        "                real_pos  = np.where(mask_line)[0][valid_idx]\n",
        "\n",
        "                # 2) iterar sobre real_pos y, si stop_on_click=True, detener tras el primer label==1\n",
        "                for j in real_pos:\n",
        "                    seq      = tuple(cand_seqs[j])\n",
        "                    titulo   = seq2title.get(seq, \"[Título desconocido]\")\n",
        "                    marcador = \"✔\" if labels[i][j] == 1 else \"✖\"\n",
        "                    print(f\"  ({marcador}) {titulo}\")\n",
        "                    if stop_on_click and labels[i][j] == 1:\n",
        "                        break\n",
        "\n",
        "                ejemplos_mostrados += 1\n",
        "                if ejemplos_mostrados >= n:\n",
        "                    return"
      ],
      "metadata": {
        "id": "OgLLWrv-Zu5W"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PAD = word2idx['<PAD>']\n",
        "idx2word = {v: k for k, v in word2idx.items()}\n",
        "seq2title = construir_seq2title(news2idx, idx2word)"
      ],
      "metadata": {
        "id": "CuC-zQSeDp9U"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\nMostrando ejemplos de perfiles y recomendaciones del modelo:\")\n",
        "mostrar_perfil_y_recomendaciones(model, val_loader, seq2title, device, n=3, hist_k=3, stop_on_click=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAwe64RvEYAZ",
        "outputId": "7cf4c762-c59e-4184-c98d-4f9b7d0cd719"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Mostrando ejemplos de perfiles y recomendaciones del modelo:\n",
            "\n",
            "--- Usuario 1 ---\n",
            "Historial (hasta 3 ítems):\n",
            " - couple didn't know why car was running strangely then they popped the hood\n",
            " - 100 outrageous things homeowners have done to their houses\n",
            " - iowa family discovers horrifying basement filled with blood\n",
            "\n",
            "Candidatos (ordenados por score):\n",
            "  (✖) 'one in a million' deer captured on camera in michigan woods\n",
            "  (✖) a decommissioned nuclear missile complex in arizona that was abandoned for decades is now on sale for 400 000\n",
            "  (✖) american outdoor to split into two firms separating its gun business\n",
            "  (✖) experts crack mystery of ancient egypt's sacred bird mummies\n",
            "  (✖) this is chick fil a's most ordered menu item\n",
            "  (✖) car of marine linked to virginia slaying is found in south carolina authorities say\n",
            "  (✖) lexus just revealed the pricing for its first ever luxury yacht and a fully loaded model will cost 4 85\n",
            "  (✔) i moved from the us to the uk here are the 8 things that surprised me the most\n",
            "\n",
            "--- Usuario 2 ---\n",
            "Historial (hasta 3 ítems):\n",
            " - curry swishes shots in warm up sequence at chase center\n",
            " - i am unapologetically raising my daughter without religion\n",
            " - meghan markle's leather pencil skirt is a powerful statement piece\n",
            "\n",
            "Candidatos (ordenados por score):\n",
            "  (✖) chuck woolery supporting trump 'pretty much destroyed my career'\n",
            "  (✖) draymond criticizes official for ejection 'don't tell me not to talk'\n",
            "  (✖) kelly dodd is engaged to boyfriend rick leventhal 'i'm beyond excited for our future together'\n",
            "  (✔) warriors' damion lee out at least two weeks\n",
            "\n",
            "--- Usuario 3 ---\n",
            "Historial (hasta 3 ítems):\n",
            " - who the houston roughnecks took in the xfl draft\n",
            " - cuba gooding jr pleads not guilty to sex misconduct claims\n",
            " - saints dbs mocked bears rb tarik cohen for being short during heated argument\n",
            "\n",
            "Candidatos (ordenados por score):\n",
            "  (✔) wynonna judd's daughter 23 released from prison 6 years early after being granted parole\n"
          ]
        }
      ]
    }
  ]
}