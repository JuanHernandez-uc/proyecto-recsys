{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "W5eMFDfIkSQQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_behaviors = pd.read_csv(\"behaviors.tsv\", sep=\"\\t\", names=['ImpressionID', 'UserID', 'Time', 'History', 'Impressions'])\n",
        "df_news = pd.read_csv(\"news.tsv\", sep=\"\\t\", names=['NewsID', 'Category', 'SubCategory', 'Title', 'Abstract', 'URL', 'TitleEntities', 'AbstractEntities'])"
      ],
      "metadata": {
        "id": "lL0Ij4TmkxK3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_behaviors.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ie1aNADnh5ZR",
        "outputId": "ceeb0628-e561-4f32-b365-cef0a102b228"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(156965, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_behaviors[\"Time\"] = pd.to_datetime(df_behaviors[\"Time\"])\n",
        "cutoff = pd.to_datetime(\"2019-11-14\")\n",
        "\n",
        "behavior_train = df_behaviors[df_behaviors[\"Time\"] < cutoff].copy()\n",
        "behavior_val   = df_behaviors[df_behaviors[\"Time\"] >= cutoff].copy()"
      ],
      "metadata": {
        "id": "aYJ9dhTEk5jN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Usando dispositivo: {device}')"
      ],
      "metadata": {
        "id": "O7FV8zuBTgN3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c79c2312-0a84-4942-a2b3-82e8c1e9a474"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usando dispositivo: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    tokens = re.findall(r\"[\\w']+\", text.lower())\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "DqZFd1fdTiUI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "longitudes = df_news[\"Title\"].dropna().apply(lambda x: len(x.split()))\n",
        "cantidad_menor_20 = (longitudes < 20).sum()\n",
        "total = len(longitudes)\n",
        "\n",
        "print(f\"Títulos con menos de 20 palabras: {cantidad_menor_20} de {total} ({cantidad_menor_20 / total:.2%})\")"
      ],
      "metadata": {
        "id": "ENnlGkbhU0dG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "171f2ac6-aafa-473f-a9f6-31d41083691f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Títulos con menos de 20 palabras: 50633 de 51282 (98.73%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
        "idx = 2 # Por <UNK> y <PAD>\n",
        "news2idx = {}  # Mapeo: news_id -> lista de índices de palabras (padded/trunc)\n",
        "max_size_title = 20"
      ],
      "metadata": {
        "id": "Cr17bxtBV52Q"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _, row in tqdm(df_news.iterrows(), total=df_news.shape[0]):\n",
        "    news_id = row[\"NewsID\"]\n",
        "    title = row[\"Title\"]\n",
        "    tokens = [] if pd.isna(title) else tokenize(title)\n",
        "    token_idxs = []\n",
        "    for w in tokens[:max_size_title]:  # truncar título largo\n",
        "        if w not in word2idx:\n",
        "            word2idx[w] = idx\n",
        "            idx += 1\n",
        "        token_idxs.append(word2idx.get(w, word2idx['<UNK>']))\n",
        "    # Rellenar con PAD si es más corto que title_max\n",
        "    if len(token_idxs) < max_size_title:\n",
        "        token_idxs += [word2idx['<PAD>']] * (max_size_title - len(token_idxs))\n",
        "    news2idx[news_id] = token_idxs"
      ],
      "metadata": {
        "id": "3ePeLaQfVduC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a1a3b12-a195-4410-a4b8-3fd688009532"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 51282/51282 [00:03<00:00, 17020.89it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(word2idx)\n",
        "print(f'Vocabulario: {vocab_size} palabras')"
      ],
      "metadata": {
        "id": "YzJUN1lvWU34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d41b0bf5-9d7e-4e8f-f2ac-028e89f367e6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulario: 37272 palabras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "for _, row in tqdm(behavior_train.iterrows(), total=behavior_train.shape[0]):\n",
        "    hist_str = row['History']\n",
        "    hist_ids = [] if pd.isna(hist_str) else [nid for nid in hist_str.split() if nid]\n",
        "    impr = row['ImpressionID']\n",
        "    imps = [] if pd.isna(row['Impressions']) else row['Impressions'].split()\n",
        "    for imp in imps:\n",
        "        if len(imp) == 0:\n",
        "            continue\n",
        "        parts = imp.split('-')\n",
        "        if len(parts) != 2:\n",
        "            continue\n",
        "        news_id, click = parts[0], parts[1]\n",
        "        label = int(click)\n",
        "        data.append((impr, hist_ids, news_id, label))"
      ],
      "metadata": {
        "id": "QoCzTq-vWccd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d00399b-bc74-48f0-a201-ca82bb6c29d7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 126695/126695 [00:18<00:00, 6773.08it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[:460000]"
      ],
      "metadata": {
        "id": "YeqSSwr0P2rT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Total de ejemplos de interacción: {len(data)}')"
      ],
      "metadata": {
        "id": "x5FWH_6lW9Le",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3574e097-68e4-4f8f-e870-ce329834c4a4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de ejemplos de interacción: 460000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_data = []\n",
        "\n",
        "for _, row in tqdm(behavior_val.iterrows(), total=behavior_val.shape[0]):\n",
        "    hist_str = row['History']\n",
        "    hist_ids = [] if pd.isna(hist_str) else [nid for nid in hist_str.split() if nid]\n",
        "    impr = row['ImpressionID']\n",
        "    imps = row['Impressions'].split()\n",
        "    for imp in imps:\n",
        "        if len(imp) == 0:\n",
        "            continue\n",
        "        parts = imp.split('-')\n",
        "        if len(parts) != 2:\n",
        "            continue\n",
        "        news_id, click = parts[0], parts[1]\n",
        "        val_data.append((impr, hist_ids, news_id, int(click)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MNHr5cLrw8H",
        "outputId": "103f7073-84ad-4240-90f7-725014b7f83f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30270/30270 [00:04<00:00, 6078.06it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_data = val_data[:120000]"
      ],
      "metadata": {
        "id": "xnh-ZDyOP-LB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Total ejemplos validación: {len(val_data)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d36pXQwyr5go",
        "outputId": "873a137a-87c9-45b6-c887-0509b535d1de"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total ejemplos validación: 120000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MINDDataset(Dataset):\n",
        "    def __init__(self, interactions, news2idx, word2idx, hist_max, title_max):\n",
        "        self.interactions = interactions\n",
        "        self.news2idx = news2idx\n",
        "        self.word2idx = word2idx\n",
        "        self.hist_max = hist_max\n",
        "        self.title_max = title_max\n",
        "    def __len__(self):\n",
        "        return len(self.interactions)\n",
        "    def __getitem__(self, idx):\n",
        "        impr, hist_ids, cand_id, label = self.interactions[idx]\n",
        "        # Truncar o pad historial\n",
        "        if len(hist_ids) > self.hist_max:\n",
        "            hist_ids = hist_ids[-self.hist_max:]\n",
        "        hist_seq = []\n",
        "        for nid in hist_ids:\n",
        "            seq = self.news2idx.get(nid, [self.word2idx['<PAD>']] * self.title_max)\n",
        "            hist_seq.append(seq)\n",
        "        if len(hist_seq) < self.hist_max:\n",
        "            pad_seq = [self.word2idx['<PAD>']] * self.title_max\n",
        "            for _ in range(self.hist_max - len(hist_seq)):\n",
        "                hist_seq.insert(0, pad_seq)\n",
        "        # Noticia candidata\n",
        "        cand_seq = self.news2idx.get(cand_id, [self.word2idx['<PAD>']] * self.title_max)\n",
        "        # Convertir a tensores\n",
        "        hist_tensor = torch.tensor(hist_seq, dtype=torch.long)\n",
        "        cand_tensor = torch.tensor(cand_seq, dtype=torch.long)\n",
        "        label_tensor = torch.tensor(label, dtype=torch.float)\n",
        "        return hist_tensor, cand_tensor, label_tensor, impr"
      ],
      "metadata": {
        "id": "IdKBDf4EXADq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    hist_list, cand_list, label_list, impr_list = zip(*batch)\n",
        "    hist_batch = torch.stack(hist_list)        # (batch, hist_max, title_max)\n",
        "    cand_batch = torch.stack(cand_list)        # (batch, title_max)\n",
        "    label_batch = torch.stack(label_list).view(-1,1)\n",
        "    impr_batch = list(impr_list)\n",
        "    return hist_batch.to(device), cand_batch.to(device), label_batch.to(device), impr_batch"
      ],
      "metadata": {
        "id": "_4Iiwna9rTP0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_hist_title = 50\n",
        "batch_size = 128"
      ],
      "metadata": {
        "id": "orQi2EU7sRZT"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MINDDataset(data, news2idx, word2idx, max_hist_title, max_size_title)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "val_dataset = MINDDataset(val_data, news2idx, word2idx, max_hist_title, max_size_title)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "HITyQvzvrUxg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = None"
      ],
      "metadata": {
        "id": "1_ScmTrZth42"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fastformer"
      ],
      "metadata": {
        "id": "nTb1pRq0SP3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionPooling(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(AttentionPooling, self).__init__()\n",
        "        self.att_fc1 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.att_fc2 = nn.Linear(hidden_size, 1)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        bz = x.shape[0]\n",
        "        e = self.att_fc1(x)\n",
        "        e = self.tanh(e)\n",
        "        alpha = self.att_fc2(e)\n",
        "        alpha = torch.exp(alpha)\n",
        "        if attn_mask is not None:\n",
        "            alpha = alpha * attn_mask.unsqueeze(2)\n",
        "        alpha = alpha / (torch.sum(alpha, dim=1, keepdim=True) + 1e-8)\n",
        "        x = torch.bmm(x.permute(0, 2, 1), alpha)\n",
        "        x = torch.reshape(x, (bz, -1))\n",
        "        return x\n",
        "\n",
        "class FastSelfAttention(nn.Module):\n",
        "  def __init__(self, hidden_size, num_attention_heads):\n",
        "      super(FastSelfAttention, self).__init__()\n",
        "\n",
        "      if hidden_size % num_attention_heads != 0:\n",
        "          raise ValueError(\n",
        "              \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "              \"heads (%d)\" %\n",
        "              (hidden_size, num_attention_heads))\n",
        "      self.attention_head_size = int(hidden_size /num_attention_heads)\n",
        "      self.num_attention_heads = num_attention_heads\n",
        "      self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "      self.input_dim= hidden_size\n",
        "\n",
        "      self.query = nn.Linear(self.input_dim, self.all_head_size)\n",
        "      self.query_att = nn.Linear(self.all_head_size, self.num_attention_heads)\n",
        "      self.key = nn.Linear(self.input_dim, self.all_head_size)\n",
        "      self.key_att = nn.Linear(self.all_head_size, self.num_attention_heads)\n",
        "      self.transform = nn.Linear(self.all_head_size, self.all_head_size)\n",
        "\n",
        "      self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "  def transpose_for_scores(self, x):\n",
        "      new_x_shape = x.size()[:-1] + (self.num_attention_heads,\n",
        "                                      self.attention_head_size)\n",
        "      x = x.view(*new_x_shape)\n",
        "      return x.permute(0, 2, 1, 3)\n",
        "\n",
        "  def forward(self, hidden_states, attention_mask):\n",
        "      # batch_size, seq_len, num_head * head_dim, batch_size, seq_len\n",
        "      batch_size, seq_len, _ = hidden_states.shape\n",
        "      mixed_query_layer = self.query(hidden_states)\n",
        "      mixed_key_layer = self.key(hidden_states)\n",
        "      # batch_size, num_head, seq_len\n",
        "      query_for_score = self.query_att(mixed_query_layer).transpose(1, 2) / self.attention_head_size**0.5\n",
        "      # add attention mask\n",
        "      if attention_mask.dim() == 2:\n",
        "        attention_mask = attention_mask.unsqueeze(1).repeat(1, self.num_attention_heads, 1)\n",
        "      query_for_score += attention_mask\n",
        "\n",
        "      # batch_size, num_head, 1, seq_len\n",
        "      query_weight = self.softmax(query_for_score).unsqueeze(2)\n",
        "\n",
        "      # batch_size, num_head, seq_len, head_dim\n",
        "      query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "      # batch_size, num_head, head_dim, 1\n",
        "      pooled_query = torch.matmul(query_weight, query_layer).transpose(1, 2).view(-1,1,self.num_attention_heads*self.attention_head_size)\n",
        "      pooled_query_repeat= pooled_query.repeat(1, seq_len,1)\n",
        "      # batch_size, num_head, seq_len, head_dim\n",
        "\n",
        "      # batch_size, num_head, seq_len\n",
        "      mixed_query_key_layer=mixed_key_layer* pooled_query_repeat\n",
        "\n",
        "      query_key_score=(self.key_att(mixed_query_key_layer)/ self.attention_head_size**0.5).transpose(1, 2)\n",
        "\n",
        "      # add attention mask\n",
        "      query_key_score +=attention_mask\n",
        "\n",
        "      # batch_size, num_head, 1, seq_len\n",
        "      query_key_weight = self.softmax(query_key_score).unsqueeze(2)\n",
        "\n",
        "      key_layer = self.transpose_for_scores(mixed_query_key_layer)\n",
        "      pooled_key = torch.matmul(query_key_weight, key_layer)\n",
        "\n",
        "      #query = value\n",
        "      weighted_value =(pooled_key * query_layer).transpose(1, 2)\n",
        "      weighted_value = weighted_value.reshape(\n",
        "          weighted_value.size()[:-2] + (self.num_attention_heads * self.attention_head_size,))\n",
        "      weighted_value = self.transform(weighted_value) + mixed_query_layer\n",
        "\n",
        "      return weighted_value"
      ],
      "metadata": {
        "id": "jiC9v85USBVK"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NewsEncoderFastformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, pretrained_emb=None):\n",
        "        super(NewsEncoderFastformer, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=word2idx['<PAD>'])\n",
        "        if pretrained_emb is not None:\n",
        "            self.embed.weight.data.copy_(pretrained_emb)\n",
        "            self.embed.weight.requires_grad = True\n",
        "        self.fastformer = FastSelfAttention(embed_dim, num_heads)\n",
        "        self.pooler = AttentionPooling(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mask = x.bool().float()\n",
        "        emb = self.embed(x)                           # (batch, seq_len, embed_dim)\n",
        "        attn_out = self.fastformer(emb, mask)         # (batch, seq_len, embed_dim)\n",
        "        news_vec = self.pooler(attn_out, mask)        # (batch, embed_dim)\n",
        "        return news_vec\n",
        "\n",
        "class UserEncoderFastformer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(UserEncoderFastformer, self).__init__()\n",
        "        self.fastformer = FastSelfAttention(embed_dim, num_heads)\n",
        "        self.pooler = AttentionPooling(embed_dim)\n",
        "\n",
        "    def forward(self, news_vecs):\n",
        "        # news_vecs: (batch, hist_size, embed_dim)\n",
        "        mask = news_vecs.abs().sum(dim=-1).bool().float()  # detect padding\n",
        "        attn_out = self.fastformer(news_vecs, mask)\n",
        "        user_vec = self.pooler(attn_out, mask)\n",
        "        return user_vec\n",
        "\n",
        "class FastformerNRMS(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, title_max, hist_max, pretrained_emb=None):\n",
        "        super(FastformerNRMS, self).__init__()\n",
        "        self.news_encoder = NewsEncoderFastformer(vocab_size, embed_dim, num_heads, pretrained_emb)\n",
        "        self.user_encoder = UserEncoderFastformer(embed_dim, num_heads)\n",
        "\n",
        "    def forward(self, hist, cand):\n",
        "        batch_size = hist.size(0)\n",
        "        hist_flat = hist.view(-1, hist.size(2))           # (batch*hist_max, title_max)\n",
        "        hist_vecs = self.news_encoder(hist_flat)          # (batch*hist_max, embed_dim)\n",
        "        hist_vecs = hist_vecs.view(batch_size, -1, hist_vecs.size(-1))  # (batch, hist_max, embed_dim)\n",
        "        user_vec = self.user_encoder(hist_vecs)           # (batch, embed_dim)\n",
        "        cand_vec = self.news_encoder(cand)                # (batch, embed_dim)\n",
        "        score = torch.sum(user_vec * cand_vec, dim=1)     # (batch,)\n",
        "        return score"
      ],
      "metadata": {
        "id": "uvrDeszx8TkQ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 300\n",
        "num_heads = 10\n",
        "lr = 0.001"
      ],
      "metadata": {
        "id": "sfrxca4d9DTB"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastformerNRMS(vocab_size, embed_dim, num_heads, max_size_title, max_hist_title,\n",
        "             pretrained_emb=embedding_matrix.to(device) if embedding_matrix is not None else None)\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "G4Q4JtGF9Dtt"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 3"
      ],
      "metadata": {
        "id": "B4VxkXsE-TlQ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ndcg_score(labels, scores, k=5):\n",
        "    order = np.argsort(scores)[::-1]\n",
        "    labels = np.array(labels)\n",
        "    dcg = 0.0\n",
        "    for i in range(min(k, len(labels))):\n",
        "        rel = labels[order[i]]\n",
        "        dcg += (2**rel - 1) / np.log2(i+2)\n",
        "    ideal = np.sort(labels)[::-1]\n",
        "    idcg = 0.0\n",
        "    for i in range(min(k, int(np.sum(labels)))):\n",
        "        idcg += 1.0 / np.log2(i+2)\n",
        "    return dcg / idcg if idcg > 0 else 0.0"
      ],
      "metadata": {
        "id": "Tq5zX8dAHOPD"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, epochs + 1):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for hist_batch, cand_batch, label_batch, _ in tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\"):\n",
        "        optimizer.zero_grad()\n",
        "        scores = model(hist_batch, cand_batch)\n",
        "        loss = criterion(scores, label_batch.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch} - Pérdida promedio: {avg_loss:.4f}\")\n",
        "    # Evaluación en validación\n",
        "    if val_loader is not None:\n",
        "        model.eval()\n",
        "        all_preds = {}\n",
        "        with torch.no_grad():\n",
        "            for hist_batch, cand_batch, label_batch, impr_batch in val_loader:\n",
        "                scores = model(hist_batch, cand_batch).cpu().numpy()\n",
        "                labels = label_batch.cpu().numpy().flatten()\n",
        "                for impr_id, s, l in zip(impr_batch, scores, labels):\n",
        "                    all_preds.setdefault(impr_id, []).append((s,l))\n",
        "        # Calcular métricas en validación\n",
        "        correct, total = 0, 0\n",
        "        ndcg5_list = []\n",
        "        for impr_id, recs in all_preds.items():\n",
        "            scores = [s for (s,l) in recs]\n",
        "            labels = [l for (s,l) in recs]\n",
        "            preds_bin = [1 if s>=0.0 else 0 for s in scores]\n",
        "            correct += sum(int(p==l) for p,l in zip(preds_bin, labels))\n",
        "            total += len(labels)\n",
        "            ndcg5_list.append(ndcg_score(labels, scores, k=5))\n",
        "        acc = correct / total if total>0 else 0\n",
        "        ndcg5 = np.mean(ndcg5_list) if ndcg5_list else 0\n",
        "        print(f\"Validación - Accuracy: {acc:.4f}, nDCG@5: {ndcg5:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LXg_BPK-WB3",
        "outputId": "78f8edda-b6f3-4834-c0d9-be5e36965072"
      },
      "execution_count": 40,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/3: 100%|██████████| 3594/3594 [10:09<00:00,  5.90it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Pérdida promedio: 0.1810\n",
            "Validación - Accuracy: 0.9402, nDCG@5: 0.2065\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/3: 100%|██████████| 3594/3594 [10:06<00:00,  5.93it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 - Pérdida promedio: 0.1736\n",
            "Validación - Accuracy: 0.9398, nDCG@5: 0.2339\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3: 100%|██████████| 3594/3594 [10:04<00:00,  5.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 - Pérdida promedio: 0.1688\n",
            "Validación - Accuracy: 0.9390, nDCG@5: 0.2289\n"
          ]
        }
      ]
    }
  ]
}